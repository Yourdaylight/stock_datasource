# Stock Data Source - å¼€å‘æŒ‡å¯¼ä¸æ–°å»ºæ’ä»¶æŒ‡å—

## ğŸ“‹ é¡¹ç›®æ€»ç»“

### æ ¸å¿ƒåŠŸèƒ½
- **Aè‚¡æ•°æ®é‡‡é›†**ï¼šé€šè¿‡ TuShare API è·å–å®Œæ•´çš„ A è‚¡æ—¥çº¿æ•°æ®
- **Schema-on-API**ï¼šæ ¹æ® API å“åº”åŠ¨æ€åˆ›å»ºå’Œæ‰©å±•è¡¨ç»“æ„
- **æ•°æ®è´¨é‡æ£€æŸ¥**ï¼šå†…ç½®å¤šå±‚æ•°æ®è´¨é‡éªŒè¯æœºåˆ¶
- **Airflow ç¼–æ’**ï¼šæ”¯æŒè‡ªåŠ¨åŒ–æ—¥æ›´å’Œå†å²å›å¡«
- **ClickHouse å­˜å‚¨**ï¼šé«˜æ€§èƒ½åˆ—å¼æ•°æ®åº“å­˜å‚¨æ—¶é—´åºåˆ—æ•°æ®

### æ¶æ„è®¾è®¡
```
TuShare API (REST)
    â†“
Plugin (Extract â†’ Validate â†’ Transform â†’ Load)
    â†“
ODS Layer (åŸå§‹æ•°æ®å­˜å‚¨)
    â†“
DM/Fact Layer (æ•°æ®é›†å¸‚/äº‹å®è¡¨)
    â†“
Metadata Layer (å…ƒæ•°æ®/å®¡è®¡æ—¥å¿—)
```

### æ•°æ®åˆ†å±‚
| å±‚çº§ | è¡¨åå‰ç¼€ | ç”¨é€” | ç‰¹ç‚¹ |
|------|---------|------|------|
| ODS | `ods_*` | åŸå§‹æ•°æ®å­˜å‚¨ | æŒ‰æœˆåˆ†åŒºï¼ŒSchema åŠ¨æ€æ¼”è¿› |
| DM | `dim_*` | ç»´åº¦è¡¨ | è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œç¼“æ…¢å˜åŒ–ç»´ |
| Fact | `fact_*` | äº‹å®è¡¨ | æ—¥çº¿æ•°æ®ï¼Œé¢„èšåˆæŒ‡æ ‡ |
| Meta | `meta_*` | å…ƒæ•°æ® | æ‘„å…¥æ—¥å¿—ã€å¤±è´¥ä»»åŠ¡ã€è´¨é‡æ£€æŸ¥ |

---

## ğŸš€ åç»­å¼€å‘æŒ‡å¯¼

### 1. å¿«é€Ÿå¼€å§‹

#### ç¯å¢ƒé…ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd stock_datasource

# å®‰è£…ä¾èµ–
uv sync

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .envï¼Œå¡«å…¥ TuShare Token å’Œ ClickHouse è¿æ¥ä¿¡æ¯
```

#### åˆå§‹åŒ–æ•°æ®åº“
```bash
# åˆ›å»ºæ•°æ®åº“å’Œæ‰€æœ‰è¡¨
uv run cli.py init-db

# åˆ›å»ºç‰¹å®šè¡¨
uv run cli.py init-db --table ods_daily
```

### 2. å¸¸ç”¨ CLI å‘½ä»¤

```bash
# è·å–ç‰¹å®šæ—¥æœŸçš„æ•°æ®
uv run cli.py ingest-daily --date 20251024

# æ‰¹é‡å›å¡«æ•°æ®
uv run cli.py backfill --start-date 20250101 --end-date 20251024

# æŸ¥çœ‹æ‘„å…¥çŠ¶æ€
uv run cli.py status --date 20251024

# è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
uv run cli.py quality-check --date 20251024

# ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
uv run cli.py report --date 20251024

# æ•°æ®è¦†ç›–ç‡æ£€æŸ¥
uv run cli.py coverage --table ods_daily

# æ¸…ç†æ—§æ•°æ®
uv run cli.py cleanup --days 30
```

### 3. é¡¹ç›®ç»“æ„å¯¼èˆª

```
src/stock_datasource/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ base_plugin.py          # æ’ä»¶åŸºç±»ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â””â”€â”€ plugin_manager.py       # æ’ä»¶ç®¡ç†å™¨
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ tushare_daily/          # æ—¥çº¿æ•°æ®æ’ä»¶
â”‚   â”œâ”€â”€ tushare_daily_basic/    # æ—¥çº¿åŸºç¡€æŒ‡æ ‡æ’ä»¶
â”‚   â”œâ”€â”€ tushare_adj_factor/     # å¤æƒå› å­æ’ä»¶
â”‚   â”œâ”€â”€ tushare_stk_limit/      # æ¶¨è·Œåœæ•°æ®æ’ä»¶
â”‚   â”œâ”€â”€ tushare_suspend_d/      # åœå¤ç‰Œæ•°æ®æ’ä»¶
â”‚   â”œâ”€â”€ tushare_stock_basic/    # è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ’ä»¶
â”‚   â””â”€â”€ tushare_trade_calendar/ # äº¤æ˜“æ—¥å†æ’ä»¶
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ database.py             # æ•°æ®åº“è¿æ¥
â”‚   â””â”€â”€ schemas.py              # è¡¨ç»“æ„å®šä¹‰
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingestion.py            # æ‘„å…¥æœåŠ¡
â”‚   â””â”€â”€ metadata.py             # å…ƒæ•°æ®æœåŠ¡
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ extractor.py            # æ•°æ®æå–å·¥å…·
â”‚   â”œâ”€â”€ loader.py               # æ•°æ®åŠ è½½å·¥å…·
â”‚   â”œâ”€â”€ quality_checks.py       # è´¨é‡æ£€æŸ¥å·¥å…·
â”‚   â””â”€â”€ logger.py               # æ—¥å¿—å·¥å…·
â””â”€â”€ dags/
    â”œâ”€â”€ daily_cn_1800.py        # æ—¥æ›´ DAG
    â”œâ”€â”€ backfill_cn_2020.py     # å›å¡« DAG
    â””â”€â”€ hk_placeholders.py      # æ¸¯è‚¡å ä½ DAG
```

### 4. æ•°æ®æµç¨‹

#### æ—¥å¸¸æ‘„å…¥æµç¨‹
```
1. è§¦å‘æ¡ä»¶ï¼šæ¯å¤© 18:00 (Asia/Shanghai)
2. æ‰§è¡Œæ­¥éª¤ï¼š
   - è·å–äº¤æ˜“æ—¥å† â†’ ç¡®å®šæ˜¯å¦ä¸ºäº¤æ˜“æ—¥
   - å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ’ä»¶çš„ run() æ–¹æ³•
   - æ¯ä¸ªæ’ä»¶æ‰§è¡Œï¼šExtract â†’ Validate â†’ Transform â†’ Load
   - è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
   - è®°å½•æ‘„å…¥æ—¥å¿—å’Œå…ƒæ•°æ®
3. è¾“å‡ºï¼šODS è¡¨ä¸­çš„åŸå§‹æ•°æ® + Fact è¡¨ä¸­çš„æ¸…æ´—æ•°æ®
```

#### å†å²å›å¡«æµç¨‹
```
1. è§¦å‘æ¡ä»¶ï¼šæ‰‹åŠ¨æ‰§è¡Œ backfill å‘½ä»¤
2. æ‰§è¡Œæ­¥éª¤ï¼š
   - è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰äº¤æ˜“æ—¥
   - æŒ‰æ—¥æœŸé¡ºåºé€æ—¥æ‰§è¡Œæ‘„å…¥æµç¨‹
   - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆå¤±è´¥æ—¥æœŸå¯é‡æ–°æ‰§è¡Œï¼‰
3. è¾“å‡ºï¼šå®Œæ•´çš„å†å²æ•°æ®
```

### 5. æ•°æ®è´¨é‡æ£€æŸ¥

ç³»ç»Ÿå†…ç½®çš„è´¨é‡æ£€æŸ¥åŒ…æ‹¬ï¼š

| æ£€æŸ¥é¡¹ | è¯´æ˜ | è§¦å‘æ¡ä»¶ |
|--------|------|---------|
| äº¤æ˜“æ—¥å¯¹é½ | éªŒè¯è®°å½•æ•°ä¸äº¤æ˜“æ—¥å†åŒ¹é… | æ¯æ—¥æ‘„å…¥å |
| ä»·æ ¼ä¸€è‡´æ€§ | éªŒè¯ OHLC å…³ç³»ï¼ˆHigh â‰¥ Open/Close â‰¥ Lowï¼‰ | æ—¥çº¿æ•°æ®æ‘„å…¥å |
| æ¶¨è·Œåœä¸€è‡´æ€§ | éªŒè¯æ¶¨è·Œåœæ•°æ®ä¸æ—¥çº¿æ•°æ®ä¸€è‡´ | æ—¥çº¿ + æ¶¨è·Œåœæ•°æ®æ‘„å…¥å |
| åœå¤ç‰Œä¸€è‡´æ€§ | éªŒè¯åœå¤ç‰Œæ•°æ®ä¸æ—¥çº¿æ•°æ®ä¸€è‡´ | æ—¥çº¿ + åœå¤ç‰Œæ•°æ®æ‘„å…¥å |

### 6. æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### API è°ƒç”¨ä¼˜åŒ–
- **é€Ÿç‡é™åˆ¶**ï¼šA è‚¡æ•°æ® 120-150 calls/minï¼ˆTuShare 2000 ç§¯åˆ†æ¡£ï¼‰
- **æ‰¹é‡è¯·æ±‚**ï¼šä½¿ç”¨ TuShare çš„åˆ†é¡µåŠŸèƒ½å‡å°‘è¯·æ±‚æ¬¡æ•°
- **ç¼“å­˜ç­–ç•¥**ï¼šè‚¡ç¥¨åŸºç¡€ä¿¡æ¯å¯ç¼“å­˜ï¼ˆå˜åŒ–ä¸é¢‘ç¹ï¼‰

#### æ•°æ®åº“ä¼˜åŒ–
- **åˆ†åŒºç­–ç•¥**ï¼šODS è¡¨æŒ‰æœˆåˆ†åŒºï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½
- **ç´¢å¼•è®¾ç½®**ï¼šåœ¨ `ts_code` å’Œ `trade_date` ä¸Šå»ºç«‹ç´¢å¼•
- **å‹ç¼©**ï¼šClickHouse è‡ªåŠ¨å‹ç¼©ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®

#### å¹¶è¡Œå¤„ç†
- **æ’ä»¶å¹¶è¡Œ**ï¼š7 ä¸ªæ’ä»¶å¯å¹¶è¡Œæ‰§è¡Œï¼ˆäº’ä¸ä¾èµ–ï¼‰
- **æ—¥æœŸå¹¶è¡Œ**ï¼šbackfill æ—¶å¯é…ç½®å¹¶è¡Œåº¦

---

## ğŸ”§ æ–°å»ºæ’ä»¶æŒ‡å—

### 1. æ’ä»¶æ¶æ„

æ¯ä¸ªæ’ä»¶æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Python åŒ…ï¼ŒåŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
plugins/my_plugin/
â”œâ”€â”€ __init__.py              # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ plugin.py                # æ’ä»¶ä¸»ç±»ï¼ˆç»§æ‰¿ BasePluginï¼‰
â”œâ”€â”€ extractor.py             # æ•°æ®æå–é€»è¾‘
â”œâ”€â”€ config.json              # æ’ä»¶é…ç½®
â”œâ”€â”€ schema.json              # è¡¨ç»“æ„å®šä¹‰
â””â”€â”€ README.md                # æ’ä»¶æ–‡æ¡£
```

### 2. å¿…éœ€çš„ä¸‰ä¸ªæ–¹æ³•

#### 2.1 `extract_data(**kwargs) -> pd.DataFrame`
**èŒè´£**ï¼šä»æ•°æ®æºè·å–åŸå§‹æ•°æ®

```python
def extract_data(self, **kwargs) -> pd.DataFrame:
    """
    ä» TuShare API è·å–æ•°æ®
    
    Args:
        **kwargs: æ’ä»¶ç‰¹å®šå‚æ•°
            - trade_date: äº¤æ˜“æ—¥æœŸ (YYYYMMDD)
            - start_date: å¼€å§‹æ—¥æœŸ
            - end_date: ç»“æŸæ—¥æœŸ
            - list_status: ä¸Šå¸‚çŠ¶æ€ (L/D/P)
    
    Returns:
        pd.DataFrame: åŸå§‹æ•°æ®
    """
    # 1. å‚æ•°éªŒè¯
    trade_date = kwargs.get('trade_date')
    if not trade_date:
        raise ValueError("trade_date is required")
    
    # 2. è°ƒç”¨ API
    data = self.extractor.extract(trade_date)
    
    # 3. åŸºç¡€å¤„ç†ï¼ˆå¯é€‰ï¼‰
    if data.empty:
        self.logger.warning(f"No data for {trade_date}")
        return pd.DataFrame()
    
    # 4. è¿”å›æ•°æ®
    return data
```

**å…³é”®ç‚¹**ï¼š
- âœ… å¿…é¡»è¿”å› `pd.DataFrame`
- âœ… å¤„ç†ç©ºæ•°æ®æƒ…å†µ
- âœ… è®°å½•æ—¥å¿—ä¾¿äºè°ƒè¯•
- âœ… ä¸è¦åœ¨æ­¤å¤„è¿›è¡Œæ•°æ®è½¬æ¢

#### 2.2 `validate_data(data: pd.DataFrame) -> bool`
**èŒè´£**ï¼šéªŒè¯æ•°æ®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§

```python
def validate_data(self, data: pd.DataFrame) -> bool:
    """
    éªŒè¯æå–çš„æ•°æ®
    
    Returns:
        bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    # 1. æ£€æŸ¥ç©ºæ•°æ®
    if data.empty:
        self.logger.warning("Empty data")
        return False
    
    # 2. æ£€æŸ¥å¿…éœ€åˆ—
    required_columns = ['ts_code', 'trade_date', 'close']
    missing = [col for col in required_columns if col not in data.columns]
    if missing:
        self.logger.error(f"Missing columns: {missing}")
        return False
    
    # 3. æ£€æŸ¥ç©ºå€¼
    null_count = data['ts_code'].isnull().sum()
    if null_count > 0:
        self.logger.error(f"Found {null_count} null values in ts_code")
        return False
    
    # 4. ä¸šåŠ¡é€»è¾‘éªŒè¯
    # ä¾‹å¦‚ï¼šä»·æ ¼å…³ç³»éªŒè¯
    invalid = data[data['high'] < data['low']]
    if len(invalid) > 0:
        self.logger.error(f"Found {len(invalid)} invalid price records")
        return False
    
    self.logger.info(f"Validation passed for {len(data)} records")
    return True
```

**å…³é”®ç‚¹**ï¼š
- âœ… æ£€æŸ¥å¿…éœ€åˆ—çš„å­˜åœ¨
- âœ… æ£€æŸ¥å…³é”®å­—æ®µçš„ç©ºå€¼
- âœ… éªŒè¯ä¸šåŠ¡é€»è¾‘çº¦æŸ
- âœ… è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

#### 2.3 `load_data(data: pd.DataFrame) -> Dict[str, Any]`
**èŒè´£**ï¼šå°†æ¸…æ´—åçš„æ•°æ®åŠ è½½åˆ°æ•°æ®åº“

```python
def load_data(self, data: pd.DataFrame) -> Dict[str, Any]:
    """
    åŠ è½½æ•°æ®åˆ°æ•°æ®åº“
    
    Args:
        data: æ¸…æ´—åçš„æ•°æ®
    
    Returns:
        Dict: åŠ è½½ç»Ÿè®¡ä¿¡æ¯
    """
    if not self.db:
        return {"status": "failed", "error": "Database not initialized"}
    
    if data.empty:
        return {"status": "no_data", "loaded_records": 0}
    
    results = {
        "status": "success",
        "tables_loaded": [],
        "total_records": 0
    }
    
    try:
        # 1. å‡†å¤‡æ•°æ®ï¼ˆç±»å‹è½¬æ¢ï¼‰
        data = self._prepare_data_for_insert('ods_my_table', data)
        
        # 2. æ·»åŠ ç³»ç»Ÿåˆ—
        data = self._add_system_columns(data)
        
        # 3. æ’å…¥æ•°æ®
        self.logger.info(f"Loading {len(data)} records into ods_my_table")
        self.db.insert_dataframe('ods_my_table', data)
        
        results['tables_loaded'].append({
            'table': 'ods_my_table',
            'records': len(data)
        })
        results['total_records'] += len(data)
        
        self.logger.info(f"Successfully loaded {len(data)} records")
        
    except Exception as e:
        self.logger.error(f"Failed to load data: {e}")
        results['status'] = 'failed'
        results['error'] = str(e)
    
    return results
```

**å…³é”®ç‚¹**ï¼š
- âœ… ä½¿ç”¨ `_prepare_data_for_insert()` è¿›è¡Œç±»å‹è½¬æ¢
- âœ… ä½¿ç”¨ `_add_system_columns()` æ·»åŠ ç‰ˆæœ¬å’Œæ‘„å…¥æ—¶é—´
- âœ… è¿”å›åŠ è½½ç»Ÿè®¡ä¿¡æ¯
- âœ… å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•

### 3. å¯é€‰çš„æ–¹æ³•

#### 3.1 `transform_data(data: pd.DataFrame) -> pd.DataFrame`
**èŒè´£**ï¼šæ•°æ®è½¬æ¢å’Œæ¸…æ´—ï¼ˆé»˜è®¤ï¼šç›´æ¥è¿”å›ï¼‰

```python
def transform_data(self, data: pd.DataFrame) -> pd.DataFrame:
    """
    è½¬æ¢æ•°æ®æ ¼å¼
    """
    # æ•°æ®ç±»å‹è½¬æ¢
    numeric_cols = ['open', 'high', 'low', 'close']
    for col in numeric_cols:
        if col in data.columns:
            data[col] = pd.to_numeric(data[col], errors='coerce')
    
    # æ—¥æœŸæ ¼å¼è½¬æ¢
    if 'trade_date' in data.columns:
        data['trade_date'] = pd.to_datetime(
            data['trade_date'], 
            format='%Y%m%d'
        ).dt.date
    
    return data
```

#### 3.2 `get_dependencies() -> List[str]`
**èŒè´£**ï¼šå£°æ˜æ’ä»¶ä¾èµ–ï¼ˆé»˜è®¤ï¼šæ— ä¾èµ–ï¼‰

```python
def get_dependencies(self) -> List[str]:
    """
    è¿”å›ä¾èµ–çš„å…¶ä»–æ’ä»¶åç§°
    """
    return ['tushare_stock_basic']  # éœ€è¦å…ˆæ‰§è¡Œ stock_basic æ’ä»¶
```

#### 3.3 `get_schedule() -> Dict[str, Any]`
**èŒè´£**ï¼šå®šä¹‰æ‰§è¡Œè®¡åˆ’ï¼ˆé»˜è®¤ï¼šæ¯æ—¥ 18:00ï¼‰

```python
def get_schedule(self) -> Dict[str, Any]:
    """
    å®šä¹‰æ’ä»¶æ‰§è¡Œè®¡åˆ’
    """
    return {
        "frequency": "daily",      # 'daily' æˆ– 'weekly'
        "time": "18:00",           # HH:MM æ ¼å¼
        "day_of_week": "monday"    # ä»…å½“ frequency='weekly' æ—¶éœ€è¦
    }
```

### 4. é…ç½®æ–‡ä»¶ (config.json)

```json
{
  "enabled": true,
  "description": "My custom data plugin",
  "rate_limit": 120,
  "timeout": 30,
  "retry_attempts": 3,
  "schedule": {
    "frequency": "daily",
    "time": "18:00"
  },
  "parameters_schema": {
    "trade_date": {
      "type": "string",
      "format": "YYYYMMDD",
      "required": true,
      "description": "Trade date"
    }
  }
}
```

### 5. è¡¨ç»“æ„å®šä¹‰ (schema.json)

```json
{
  "table_name": "ods_my_table",
  "table_type": "ods",
  "engine": "ReplacingMergeTree",
  "engine_params": {
    "order_by": ["ts_code", "trade_date"],
    "partition_by": "toYYYYMM(trade_date)"
  },
  "columns": [
    {
      "name": "ts_code",
      "data_type": "String",
      "nullable": false,
      "comment": "Stock code"
    },
    {
      "name": "trade_date",
      "data_type": "Date",
      "nullable": false,
      "comment": "Trade date"
    },
    {
      "name": "close",
      "data_type": "Float64",
      "nullable": true,
      "comment": "Close price"
    },
    {
      "name": "version",
      "data_type": "Int64",
      "nullable": false,
      "comment": "Data version (timestamp)"
    },
    {
      "name": "_ingested_at",
      "data_type": "DateTime",
      "nullable": false,
      "comment": "Ingestion timestamp"
    }
  ]
}
```

### 6. å®Œæ•´çš„æ’ä»¶ç¤ºä¾‹

```python
# plugins/my_plugin/plugin.py
import pandas as pd
from typing import Dict, Any
from datetime import datetime
from pathlib import Path
import json

from stock_datasource.plugins import BasePlugin
from .extractor import extractor


class MyPlugin(BasePlugin):
    """My custom data plugin."""
    
    @property
    def name(self) -> str:
        return "my_plugin"
    
    @property
    def description(self) -> str:
        return "My custom data source"
    
    def extract_data(self, **kwargs) -> pd.DataFrame:
        """Extract data from source."""
        trade_date = kwargs.get('trade_date')
        if not trade_date:
            raise ValueError("trade_date is required")
        
        self.logger.info(f"Extracting data for {trade_date}")
        data = extractor.extract(trade_date)
        
        if data.empty:
            self.logger.warning(f"No data for {trade_date}")
            return pd.DataFrame()
        
        self.logger.info(f"Extracted {len(data)} records")
        return data
    
    def validate_data(self, data: pd.DataFrame) -> bool:
        """Validate extracted data."""
        if data.empty:
            return False
        
        required_cols = ['ts_code', 'trade_date']
        if not all(col in data.columns for col in required_cols):
            self.logger.error("Missing required columns")
            return False
        
        return True
    
    def transform_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Transform data."""
        # ç±»å‹è½¬æ¢
        if 'trade_date' in data.columns:
            data['trade_date'] = pd.to_datetime(
                data['trade_date'], 
                format='%Y%m%d'
            ).dt.date
        
        return data
    
    def load_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Load data into database."""
        if not self.db or data.empty:
            return {"status": "no_data", "loaded_records": 0}
        
        try:
            # å‡†å¤‡æ•°æ®
            data = self._prepare_data_for_insert('ods_my_table', data)
            data = self._add_system_columns(data)
            
            # æ’å…¥æ•°æ®
            self.db.insert_dataframe('ods_my_table', data)
            
            return {
                "status": "success",
                "tables_loaded": [{"table": "ods_my_table", "records": len(data)}],
                "total_records": len(data)
            }
        except Exception as e:
            self.logger.error(f"Load failed: {e}")
            return {"status": "failed", "error": str(e)}
```

### 7. æ–°å»ºæ’ä»¶çš„æ­¥éª¤

#### æ­¥éª¤ 1ï¼šåˆ›å»ºæ’ä»¶ç›®å½•
```bash
mkdir -p src/stock_datasource/plugins/my_plugin
cd src/stock_datasource/plugins/my_plugin
```

#### æ­¥éª¤ 2ï¼šåˆ›å»ºå¿…éœ€æ–‡ä»¶
```bash
touch __init__.py plugin.py extractor.py config.json schema.json README.md
```

#### æ­¥éª¤ 3ï¼šå®ç°æ’ä»¶ç±»
ç¼–è¾‘ `plugin.py`ï¼Œç»§æ‰¿ `BasePlugin` å¹¶å®ç°ä¸‰ä¸ªå¿…éœ€æ–¹æ³•ã€‚

#### æ­¥éª¤ 4ï¼šå®ç°æ•°æ®æå–å™¨
ç¼–è¾‘ `extractor.py`ï¼Œå®ç°å…·ä½“çš„ API è°ƒç”¨é€»è¾‘ã€‚

#### æ­¥éª¤ 5ï¼šé…ç½®æ’ä»¶
ç¼–è¾‘ `config.json` å’Œ `schema.json`ã€‚

#### æ­¥éª¤ 6ï¼šæ³¨å†Œæ’ä»¶
åœ¨ `__init__.py` ä¸­å¯¼å‡ºæ’ä»¶ç±»ï¼š
```python
from .plugin import MyPlugin

__all__ = ['MyPlugin']
```

#### æ­¥éª¤ 7ï¼šæµ‹è¯•æ’ä»¶
```bash
# ç›´æ¥è¿è¡Œæ’ä»¶
python src/stock_datasource/plugins/my_plugin/plugin.py --date 20251024

# æˆ–é€šè¿‡ CLI
uv run cli.py ingest-daily --date 20251024
```

### 8. æ–°å»ºæ’ä»¶çš„æ³¨æ„äº‹é¡¹

#### âš ï¸ å¿…é¡»éµå®ˆçš„è§„èŒƒ

1. **å‘½åè§„èŒƒ**
   - æ’ä»¶åç§°ï¼šå°å†™ + ä¸‹åˆ’çº¿ï¼ˆå¦‚ `my_plugin`ï¼‰
   - ç±»åç§°ï¼šPascalCaseï¼ˆå¦‚ `MyPlugin`ï¼‰
   - è¡¨åç§°ï¼š`ods_` å‰ç¼€ + æ’ä»¶åç§°

2. **æ•°æ®ç±»å‹**
   - å§‹ç»ˆè¿”å› `pd.DataFrame`
   - ä½¿ç”¨ ClickHouse æ”¯æŒçš„æ•°æ®ç±»å‹
   - æ—¥æœŸå­—æ®µä½¿ç”¨ `Date` ç±»å‹

3. **é”™è¯¯å¤„ç†**
   - æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
   - è¿”å›ç»“æ„åŒ–çš„é”™è¯¯ä¿¡æ¯
   - ä¸è¦è®©å¼‚å¸¸ä¼ æ’­åˆ°ä¸Šå±‚

4. **æ—¥å¿—è®°å½•**
   - ä½¿ç”¨ `self.logger` è®°å½•æ—¥å¿—
   - è®°å½•å…³é”®æ­¥éª¤å’Œé”™è¯¯ä¿¡æ¯
   - ä¾¿äºé—®é¢˜æ’æŸ¥

5. **ç³»ç»Ÿåˆ—**
   - å¿…é¡»æ·»åŠ  `version` å’Œ `_ingested_at` åˆ—
   - ä½¿ç”¨ `_add_system_columns()` æ–¹æ³•

#### âš ï¸ å¸¸è§é”™è¯¯

| é”™è¯¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| `ModuleNotFoundError` | æ’ä»¶æœªæ­£ç¡®æ³¨å†Œ | æ£€æŸ¥ `__init__.py` å¯¼å…¥ |
| `KeyError: 'ts_code'` | ç¼ºå°‘å¿…éœ€åˆ— | åœ¨ validate_data ä¸­æ£€æŸ¥åˆ— |
| `TypeError: unsupported operand type` | æ•°æ®ç±»å‹ä¸åŒ¹é… | åœ¨ transform_data ä¸­è¿›è¡Œç±»å‹è½¬æ¢ |
| `Connection refused` | æ•°æ®åº“è¿æ¥å¤±è´¥ | æ£€æŸ¥ ClickHouse é…ç½®å’Œè¿æ¥ |
| `API rate limit exceeded` | è¯·æ±‚è¿‡äºé¢‘ç¹ | è°ƒæ•´ `rate_limit` é…ç½® |

#### âš ï¸ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ‰¹é‡å¤„ç†**
   - ä½¿ç”¨ TuShare çš„åˆ†é¡µåŠŸèƒ½
   - ä¸€æ¬¡æ€§è·å–å¤šä¸ªäº¤æ˜“æ—¥çš„æ•°æ®

2. **ç¼“å­˜ç­–ç•¥**
   - ç¼“å­˜ä¸ç»å¸¸å˜åŒ–çš„æ•°æ®ï¼ˆå¦‚è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼‰
   - ä½¿ç”¨ Redis æˆ–æœ¬åœ°æ–‡ä»¶ç¼“å­˜

3. **å¹¶è¡Œå¤„ç†**
   - åˆ©ç”¨å¤šçº¿ç¨‹è·å–å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®
   - æ³¨æ„ API é€Ÿç‡é™åˆ¶

4. **æ•°æ®å‹ç¼©**
   - ClickHouse è‡ªåŠ¨å‹ç¼©ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
   - è€ƒè™‘ä½¿ç”¨ LZ4 æˆ– ZSTD å‹ç¼©ç®—æ³•

### 9. æµ‹è¯•æ’ä»¶

#### å•å…ƒæµ‹è¯•
```python
# tests/test_my_plugin.py
import pytest
import pandas as pd
from stock_datasource.plugins.my_plugin import MyPlugin


def test_extract_data():
    """Test data extraction."""
    plugin = MyPlugin()
    data = plugin.extract_data(trade_date='20251024')
    
    assert isinstance(data, pd.DataFrame)
    assert not data.empty
    assert 'ts_code' in data.columns


def test_validate_data():
    """Test data validation."""
    plugin = MyPlugin()
    
    # Valid data
    valid_data = pd.DataFrame({
        'ts_code': ['000001.SZ'],
        'trade_date': ['20251024'],
        'close': [10.5]
    })
    assert plugin.validate_data(valid_data) == True
    
    # Invalid data (missing column)
    invalid_data = pd.DataFrame({
        'ts_code': ['000001.SZ']
    })
    assert plugin.validate_data(invalid_data) == False


def test_load_data():
    """Test data loading."""
    plugin = MyPlugin()
    data = pd.DataFrame({
        'ts_code': ['000001.SZ'],
        'trade_date': ['20251024'],
        'close': [10.5]
    })
    
    result = plugin.load_data(data)
    assert result['status'] == 'success'
    assert result['total_records'] > 0
```

#### é›†æˆæµ‹è¯•
```bash
# è¿è¡Œå®Œæ•´çš„ ETL æµç¨‹
uv run cli.py ingest-daily --date 20251024

# æ£€æŸ¥æ•°æ®æ˜¯å¦æˆåŠŸåŠ è½½
uv run cli.py status --date 20251024

# è¿è¡Œè´¨é‡æ£€æŸ¥
uv run cli.py quality-check --date 20251024
```

---

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### 1. æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹æœ€è¿‘çš„æ—¥å¿—
tail -f logs/stock_datasource.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/errors.log

# æœç´¢ç‰¹å®šæ’ä»¶çš„æ—¥å¿—
grep "my_plugin" logs/stock_datasource.log
```

### 2. æ•°æ®è´¨é‡ç›‘æ§
```bash
# æŸ¥çœ‹ç‰¹å®šæ—¥æœŸçš„è´¨é‡æ£€æŸ¥ç»“æœ
uv run cli.py quality-check --date 20251024

# æŸ¥çœ‹æ•°æ®è¦†ç›–ç‡
uv run cli.py coverage --table ods_daily

# ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
uv run cli.py report --date 20251024
```

### 3. æ€§èƒ½ç›‘æ§
- ç›‘æ§ API è°ƒç”¨æ¬¡æ•°å’Œå“åº”æ—¶é—´
- ç›‘æ§æ•°æ®åº“æ’å…¥æ€§èƒ½
- ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ

---

## ğŸ”— ç›¸å…³èµ„æº

- **TuShare æ–‡æ¡£**ï¼šhttps://tushare.pro/
- **ClickHouse æ–‡æ¡£**ï¼šhttps://clickhouse.com/docs/
- **Airflow æ–‡æ¡£**ï¼šhttps://airflow.apache.org/docs/
- **Pandas æ–‡æ¡£**ï¼šhttps://pandas.pydata.org/docs/

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æ·»åŠ æ–°çš„æ•°æ®æºï¼Ÿ
A: æŒ‰ç…§"æ–°å»ºæ’ä»¶æŒ‡å—"åˆ›å»ºæ–°çš„æ’ä»¶ï¼Œå®ç° `extract_data()`ã€`validate_data()` å’Œ `load_data()` ä¸‰ä¸ªæ–¹æ³•ã€‚

### Q2: å¦‚ä½•ä¿®æ”¹è¡¨ç»“æ„ï¼Ÿ
A: ç¼–è¾‘ `schema.json` æ–‡ä»¶ï¼Œæ·»åŠ æˆ–ä¿®æ”¹åˆ—å®šä¹‰ã€‚ClickHouse æ”¯æŒ ALTER TABLE æ“ä½œï¼Œå¯ä»¥åŠ¨æ€æ‰©å±•è¡¨ç»“æ„ã€‚

### Q3: å¦‚ä½•å¤„ç† API é™æµï¼Ÿ
A: è°ƒæ•´ `config.json` ä¸­çš„ `rate_limit` å‚æ•°ï¼Œæˆ–åœ¨ `extract_data()` ä¸­æ·»åŠ é‡è¯•é€»è¾‘ã€‚

### Q4: å¦‚ä½•è°ƒè¯•æ’ä»¶ï¼Ÿ
A: ä½¿ç”¨ `--verbose` æ ‡å¿—è¿è¡Œ CLI å‘½ä»¤ï¼ŒæŸ¥çœ‹è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºã€‚

### Q5: å¦‚ä½•å¤„ç†æ•°æ®è´¨é‡é—®é¢˜ï¼Ÿ
A: åœ¨ `validate_data()` ä¸­æ·»åŠ æ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼Œæˆ–åœ¨ `transform_data()` ä¸­è¿›è¡Œæ•°æ®æ¸…æ´—ã€‚

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£å’Œç¤ºä¾‹
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
3. æäº¤ Issue æˆ– Pull Request
